{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "from src.models.AlexNet import AlexNet\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.AlexNet_LowRank import getBase, AlexNet_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore, evaluate_compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"/volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "\n",
    "# Set up training data:\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    # Reintroduce the 2000 datapoints model has not seen before.\n",
    "    trainset.data = trainset.data.clone()[-2000:-1000]\n",
    "    trainset.targets = trainset.targets.clone()[-2000:-1000]\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    testset.data = trainset.data[-1000:]\n",
    "    testset.targets = trainset.targets[-1000:]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass loading dataset using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up save location on HHD.\n",
    "SAVE_LOC = HDFP + \"/demo\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSED_LAYERS = [\"classifier.1.weight\", \"classifier.4.weight\"] # Set up layers to decompose\n",
    "RANK = -1 # -1 => default rank of min(min(n, m), 8)\n",
    "SCALING = -1 # -1 => default LoRA scaling of 0.5\n",
    "BRANCH_ACC = \"0.8072\" # Set up branching point\n",
    "\n",
    "# Set up weights for original AlexNet model\n",
    "original = AlexNet()\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Load from \"branch point\"\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "# Construct LoRA model from original model.\n",
    "BASEPATH = SAVE_LOC + \"/lora_base.pt\" # Extract the LoRA bases at branchpoint.\n",
    "\n",
    "# Note that this BASEPATH is only needed for the superstep - LoRA, but for now superstep-LoRA is simulated by restoring from\n",
    "# a local version of the state stored in the ipynb memory instead of from the HHD.\n",
    "# Will create a fully functional superstep / restore mechanism in future iterations of the mechanism.\n",
    "\n",
    "w, b = getBase(original, BASEPATH)\n",
    "model = AlexNet_LowRank(w, b, rank = RANK) # Create our low-rank model.\n",
    "load_sd_decomp(torch.load(BRANCH_LOC), model, DECOMPOSED_LAYERS)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_0/base_model.pt\n",
      "Epoch: 0, Iteration: 1\n",
      "Saving Checkpoint lc_checkpoint_0.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 2\n",
      "Saving Checkpoint lc_checkpoint_1.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 3\n",
      "Saving Checkpoint lc_checkpoint_2.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 4\n",
      "Saving Checkpoint lc_checkpoint_3.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 5\n",
      "Saving Checkpoint lc_checkpoint_4.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "model accuracy: 0.905\n",
      "Epoch: 0, Iteration: 6\n",
      "Saving Checkpoint lc_checkpoint_5.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 7\n",
      "Saving Checkpoint lc_checkpoint_6.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 8\n",
      "Saving Checkpoint lc_checkpoint_7.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 9\n",
      "Saving Checkpoint lc_checkpoint_8.pt @ /volumes/Ultra Touch/demo/set_0\n",
      "Epoch: 0, Iteration: 10\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_1/base_model.pt\n",
      "model accuracy: 0.86\n",
      "Epoch: 0, Iteration: 11\n",
      "Saving Checkpoint lc_checkpoint_0.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 12\n",
      "Saving Checkpoint lc_checkpoint_1.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 13\n",
      "Saving Checkpoint lc_checkpoint_2.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 14\n",
      "Saving Checkpoint lc_checkpoint_3.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 15\n",
      "Saving Checkpoint lc_checkpoint_4.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "model accuracy: 0.87\n",
      "Epoch: 0, Iteration: 16\n",
      "Saving Checkpoint lc_checkpoint_5.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 17\n",
      "Saving Checkpoint lc_checkpoint_6.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 18\n",
      "Saving Checkpoint lc_checkpoint_7.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 19\n",
      "Saving Checkpoint lc_checkpoint_8.pt @ /volumes/Ultra Touch/demo/set_1\n",
      "Epoch: 0, Iteration: 20\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_2/base_model.pt\n",
      "model accuracy: 0.91\n",
      "Epoch: 0, Iteration: 21\n",
      "Saving Checkpoint lc_checkpoint_0.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 22\n",
      "Saving Checkpoint lc_checkpoint_1.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 23\n",
      "Saving Checkpoint lc_checkpoint_2.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 24\n",
      "Saving Checkpoint lc_checkpoint_3.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 25\n",
      "Saving Checkpoint lc_checkpoint_4.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "model accuracy: 0.914\n",
      "Epoch: 0, Iteration: 26\n",
      "Saving Checkpoint lc_checkpoint_5.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 27\n",
      "Saving Checkpoint lc_checkpoint_6.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 28\n",
      "Saving Checkpoint lc_checkpoint_7.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 29\n",
      "Saving Checkpoint lc_checkpoint_8.pt @ /volumes/Ultra Touch/demo/set_2\n",
      "Epoch: 0, Iteration: 30\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_3/base_model.pt\n",
      "model accuracy: 0.908\n",
      "Epoch: 0, Iteration: 31\n",
      "Saving Checkpoint lc_checkpoint_0.pt @ /volumes/Ultra Touch/demo/set_3\n",
      "Epoch: 1, Iteration: 0\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_4/base_model.pt\n",
      "Epoch: 1, Iteration: 1\n",
      "Saving Checkpoint lc_checkpoint_0.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 2\n",
      "Saving Checkpoint lc_checkpoint_1.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 3\n",
      "Saving Checkpoint lc_checkpoint_2.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 4\n",
      "Saving Checkpoint lc_checkpoint_3.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 5\n",
      "Saving Checkpoint lc_checkpoint_4.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "model accuracy: 0.921\n",
      "Epoch: 1, Iteration: 6\n",
      "Saving Checkpoint lc_checkpoint_5.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 7\n",
      "Saving Checkpoint lc_checkpoint_6.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 8\n",
      "Saving Checkpoint lc_checkpoint_7.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 9\n",
      "Saving Checkpoint lc_checkpoint_8.pt @ /volumes/Ultra Touch/demo/set_4\n",
      "Epoch: 1, Iteration: 10\n",
      "saving full base model @ /volumes/Ultra Touch/demo/set_5/base_model.pt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 101\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# Evaluation on testing set\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Restoration based on previous restored model, for now we do a lazy restore directly on current \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# before running this lazy restore process on the generated base, so it essentially just skips the base\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;66;03m# construction process in a standard lc-lora restoration.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     restored_model \u001b[38;5;241m=\u001b[39m lazy_restore(base, base_decomp, bias, AlexNet(), \n\u001b[1;32m     99\u001b[0m                                   original\u001b[38;5;241m.\u001b[39mstate_dict(), DECOMPOSED_LAYERS, \n\u001b[1;32m    100\u001b[0m                                   rank \u001b[38;5;241m=\u001b[39m RANK, scaling \u001b[38;5;241m=\u001b[39m SCALING)\n\u001b[0;32m--> 101\u001b[0m     restored_accuracy\u001b[38;5;241m.\u001b[39mappend(\u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrestored_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Projects/loBranch/src/utils/utils.py:17\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[0;34m(model, test_ds)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, label \u001b[38;5;129;01min\u001b[39;00m test_ds:\n\u001b[0;32m---> 17\u001b[0m         _, opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m         t_dp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m         t_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (opt \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Projects/loBranch/src/models/AlexNet.py:31\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m32\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m12\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m12\u001b[39m)\n\u001b[1;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "restored_accuracy = [] # We store our restoration accuracy here.\n",
    "\n",
    "current_iter = 0 # Current iter represents the version of the model with respect to the super step.\n",
    "current_set = 0 # Represents a set of checkpoints (9 default iterations + 1 super step).\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "for epch in range(20):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "\n",
    "        # ==========================\n",
    "        # Compressing the Model\n",
    "        # ==========================\n",
    "        \n",
    "        set_path = \"/set_{}\".format(current_set) # Set up file directory for the current set. (10 models + 1 superstep)\n",
    "        if not os.path.exists(SAVE_LOC + set_path):\n",
    "            os.makedirs(SAVE_LOC + set_path)\n",
    "\n",
    "        if i == 0 and epch == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "        else:\n",
    "            if i % 10 == 0: \n",
    "                # super step process (every 10 iterations)\n",
    "                new_model = AlexNet()\n",
    "\n",
    "                # TODO: construct non-lazy restore from branchpoint (base lora weights) as well as lora supersteps.\n",
    "                # For now the LoRA super step process is simulated via lazy_restore and the weights dictionary kept in ipynb memory.\n",
    "                new_model = lazy_restore(base, base_decomp, bias, AlexNet(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "\n",
    "                # Changing the previous \"original model\" to aid the lazy restore (have only conducted \n",
    "                # restore lazily during evaluation, reason given below).\n",
    "                original = new_model\n",
    "                \n",
    "                # Increment current set id & iteration id.\n",
    "                current_set += 1\n",
    "                current_iter = 0\n",
    "\n",
    "                set_path = \"/set_{}\".format(current_set)\n",
    "                if not os.path.exists(SAVE_LOC + set_path):\n",
    "                    os.makedirs(SAVE_LOC + set_path)\n",
    "                \n",
    "                # Rebuilding LoRA layers => reset model!\n",
    "                w, b = getBase(original)\n",
    "                model = AlexNet_LowRank(w, b, rank = RANK)\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "\n",
    "                # Extract new base + save current model as super step.\n",
    "                base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), \n",
    "                                                       DECOMPOSED_LAYERS, restoring=False)\n",
    "\n",
    "            else:\n",
    "                # Delta-compression (Non-superstep)\n",
    "                \n",
    "                delta, decomp_delta, bias = lc.generate_delta(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                \n",
    "                # Saving checkpoint\n",
    "                lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                \"/set_{}\".format(current_set))\n",
    "    \n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                current_iter += 1\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i != 0  and i % 5 == 0: # Evaluation on testing set\n",
    "\n",
    "            # Restoration based on previous restored model, for now we do a lazy restore directly on current \n",
    "            # base because iterations across epochs might not align with % 10 superstep since this restoration\n",
    "            # is meant to be taken without respect to epoch.\n",
    "            # But in deployment, users define a set id and checkpoint id to generate the current base from \n",
    "            # before running this lazy restore process on the generated base, so it essentially just skips the base\n",
    "            # construction process in a standard lc-lora restoration.\n",
    "\n",
    "            restored_model = lazy_restore(base, base_decomp, bias, AlexNet(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

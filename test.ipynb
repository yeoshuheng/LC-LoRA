{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "from src.models.AlexNet import AlexNet\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.AlexNet_LowRank import getBase, AlexNet_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"/volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    trainset.data = trainset.data\n",
    "    trainset.targets = trainset.targets\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "    testset.data = testset.data\n",
    "    testset.targets = testset.targets\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()\n",
    "\n",
    "SAVE_LOC = HDFP + \"/lobranch/lobranch\"\n",
    "SAVE_LOC_FULL = HDFP + \"/lobranch/full\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "if not os.path.exists(SAVE_LOC_FULL):\n",
    "    os.makedirs(SAVE_LOC_FULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSED_LAYERS = [\"classifier.1.weight\", \"classifier.4.weight\"]\n",
    "\n",
    "# Set up weights for original AlexNet model\n",
    "original = AlexNet()\n",
    "model_original = AlexNet()\n",
    "\n",
    "# Load from \"branch point\"\n",
    "BRANCH_LOC = HDFP + \"/sim-test/alexnet/full/model-0.522.pt\"\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "w, b = getBase(model_original)\n",
    "model = AlexNet_LowRank(w, b)\n",
    "load_sd_decomp(torch.load(BRANCH_LOC), model, DECOMPOSED_LAYERS)\n",
    "learning_rate = 0.01\n",
    "rank = 3\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer_full = torch.optim.SGD(model_original.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "testing on first load:\n",
      "model accuracy: 0.5219\n",
      "Epoch: 0, Iteration: 1\n",
      "Saving Checkpoint lc_checkpoint_1.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.6573\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6233\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6506\n",
      "Epoch: 0, Iteration: 2\n",
      "Saving Checkpoint lc_checkpoint_2.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.7063\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6738\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6861\n",
      "Epoch: 0, Iteration: 3\n",
      "Saving Checkpoint lc_checkpoint_3.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.6917\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6591\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6424\n",
      "Epoch: 0, Iteration: 4\n",
      "Saving Checkpoint lc_checkpoint_4.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.7013\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6708\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6736\n",
      "Epoch: 0, Iteration: 5\n",
      "Saving Checkpoint lc_checkpoint_5.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.7038\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6657\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.7009\n",
      "Epoch: 0, Iteration: 6\n",
      "Saving Checkpoint lc_checkpoint_6.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.708\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6931\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6312\n",
      "Epoch: 0, Iteration: 7\n",
      "Saving Checkpoint lc_checkpoint_7.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.6988\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6799\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6347\n",
      "Epoch: 0, Iteration: 8\n",
      "Saving Checkpoint lc_checkpoint_8.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.7056\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.6726\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6785\n",
      "Epoch: 0, Iteration: 9\n",
      "Saving Checkpoint lc_checkpoint_9.pt @ /volumes/Ultra Touch/lobranch/lobranch\n",
      "testing on decomposed alexnet:\n",
      "model accuracy: 0.7205\n",
      "testing on checkpointed alexnet:\n",
      "model accuracy: 0.7144\n",
      "testing on original alexnet:\n",
      "model accuracy: 0.6561\n",
      "Epoch: 0, Iteration: 10\n"
     ]
    }
   ],
   "source": [
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "\n",
    "for epch in range(1):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "\n",
    "        if i == 10: # end test\n",
    "            break\n",
    "\n",
    "        if i == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights(model, SAVE_LOC, DECOMPOSED_LAYERS)\n",
    "            print(\"testing on first load:\")\n",
    "            evaluate_accuracy(model, test_loader)\n",
    "        else:\n",
    "            # Delta-compression\n",
    "            delta, new_base, decomp_delta, new_base_decomp, bias = lc.generate_delta(base, \n",
    "                                                            base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "            compressed_delta, compressed_dcomp_delta = lc.compress_delta(delta, decomp_delta)\n",
    "            \n",
    "            # Saving checkpoint\n",
    "            lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, i, SAVE_LOC)\n",
    "  \n",
    "            base = new_base # Replace base with latest for delta to accumulate.\n",
    "            base_decomp = new_base_decomp\n",
    "        \n",
    "\n",
    "        if i != 0:\n",
    "            print(\"testing on decomposed alexnet:\")\n",
    "            decomposed_full_accuracy.append(evaluate_accuracy(model, test_loader))\n",
    "            print(\"testing on checkpointed alexnet:\")\n",
    "            restored_model = lazy_restore(base, base_decomp, bias, AlexNet(), 3, \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS)\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))\n",
    "\n",
    "        torch.save(model_original.state_dict(), SAVE_LOC_FULL + \"/base_model_{}.pt\".format(i))\n",
    "        \n",
    "        if i != 0:\n",
    "            # Evaluation\n",
    "            print(\"testing on original alexnet:\")\n",
    "            full_accuracy.append(evaluate_accuracy(model_original, test_loader))\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # ======================\n",
    "        # Training on Full Model\n",
    "        # ======================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_full.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs_full = model_original(inputs)\n",
    "        loss_full = torch.nn.functional.cross_entropy(outputs_full,labels)\n",
    "        loss_full.backward()\n",
    "        optimizer_full.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
